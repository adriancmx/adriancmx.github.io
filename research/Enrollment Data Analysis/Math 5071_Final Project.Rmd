---
title: "Math 5071_Final Project"
author: "Adrian Cao"
date: "2023-12-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyr)
library(MASS)
library(robustbase)
library(quantreg)
library(leaps)
library(dplyr)
library(reshape2)
library(stringr)
library(nnet)
```

## Data Preparation

```{r}
enrollment <- read.csv("C:/Users/a_i_b/Downloads/enrollment_2017_2022.csv")

head(enrollment)
```

First, we identify that the dataset contains 11 variable columns:

\begin{enumerate}
\item X: Appears to be an index or identifier for each record.
\item StdtId: Encrypted Student ID.
\item Sem: Semester of enrollment, including both year and semester code (05 for Fall, 02 for Spring).
\item Crs: Course number.
\item Sec: Section number of the course.
\item SecType: Section type, where 'S' indicates a standard lecture section.
\item Units: The number of units for the course.
\item DeanCd: Title of the course.
\item GradeOpt: Grade option selected by the student.
\item PrimeDiv: Primary department of the student.
\item YRLevel: Year level of the student.
\end{enumerate}

Our analysis will focus on 100- and 200-level courses and exclude discussion sections. For this, we need to filter the data based on the Crs and SecType columns. We will also investigate the enrollment trends over the years and specifically look at any changes during the COVID-19 pandemic period.

Here, I used 'dplyr' package to do the filter, following these steps:
\begin{enumerate}
\item Filter the dataset for 100- and 200-level courses.
\item Exclude discussion sections (SecType != 'S').
\item Analyze enrollment trends over the years and during the COVID period.
\end{enumerate}
```{r}
# Filter the data for 100- and 200-level courses and SecType 'S'
filtered_enrollment <- enrollment %>%
  filter(Crs >= 100 & Crs < 300 & SecType == 'S')

# Display the first few rows of the filtered data
head(filtered_enrollment)
```
With this data set holding only the elementary courses and excluding the discussion session, we may would to do further clean the data to see if there is any missing values or if we need to encode any categorical variable if possible.
```{r}
library(VIM)
aggr_plot <- aggr(filtered_enrollment, col=c('navyblue','yellow'), numbers=TRUE, sortVars=TRUE, labels=names(filtered_enrollment), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
summary(filtered_enrollment$YRLevel)
filtered_enrollment <- na.omit(filtered_enrollment, cols = "YRLevel")
sum(is.na(filtered_enrollment$YRLevel))
```
So just as we see in the summary statistics for the whole data set, there are some missing values for the year level. I just remove the missing values as there are very little percentage. 

## Exploratory Data Analysis

Before diving deep into the model building, we start with some exploratory data analysis to better understand the distribution and charateristics of the data, including trends over time, enrollment patterns by course, department, year levels, and COVID period.

**Enrollment Distribution by Years**
```{r}
# Example: Convert 'Sem' into separate 'Year' and 'Semester' variables
filtered_enrollment <- filtered_enrollment %>%
  mutate(Year = as.integer(substr(Sem, 1, 4)),
         Semester = as.integer(substr(Sem, 5, 6)))
# Adjusting semester extraction
filtered_enrollment$Semester <- ifelse(filtered_enrollment$Semester == '5', 'Fall', 'Spring')

# Group data by Year and Semester to get the count of enrollments
enrollment_trend <- filtered_enrollment %>%
  count(Year, Semester) %>%
  rename(Enrollments = n)

# Plotting the enrollment trend over the years
ggplot(enrollment_trend, aes(x = Year, y = Enrollments, color = Semester)) +
  geom_line() +
  theme_minimal() +
  labs(title = 'Enrollment Trend Over the Years',
       x = 'Year',
       y = 'Number of Enrollments')
```
```{r}
# Group data by Year, Semester, and Course to get the count of enrollments
course_enrollment_trend <- filtered_enrollment %>%
  count(Year, Semester, Crs) %>%
  rename(Enrollments = n)

# Plotting the enrollment trend over the years for different courses by Semester
ggplot(course_enrollment_trend, aes(x = Year, y = Enrollments, group = interaction(Crs, Semester), color = Semester)) +
  geom_line() +
  facet_wrap(~ Crs, scales = 'free_y') +  # Separate plots for each course
  theme_minimal() +
  labs(title = 'Enrollment Trend by Course and Semester Over the Years',
       x = 'Year',
       y = 'Number of Enrollments') +
  theme(legend.position = "bottom")
```


**Enrollment Distribution Across Different Courses**
```{r}
# Enrollment distribution across different courses
course_enrollment <- filtered_enrollment %>%
  count(Crs) %>%
  arrange(desc(n)) %>%
  rename(Enrollments = n)

# Plotting the courses by enrollment
ggplot(course_enrollment, aes(x = Crs, y = Enrollments)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = 'Courses by Enrollment',
       x = 'Course Number',
       y = 'Number of Enrollments')
```
**Enrollment Distribution Across Different Department**
```{r}
# Enrollment trends by primary department
department_enrollment <- filtered_enrollment %>%
  count(PrimeDiv) %>%
  arrange(desc(n)) %>%
  rename(Enrollments = n)

# Plotting the department-wise enrollment
ggplot(department_enrollment, aes(x = Enrollments, y = reorder(PrimeDiv, Enrollments))) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = 'Enrollment Trends by Primary Department',
       x = 'Number of Enrollments',
       y = 'Primary Department')
```
**Enrollment Variations by Student Year Level**
```{r}
# Enrollment variations by student year level
year_level_enrollment <- filtered_enrollment %>%
  count(YRLevel) %>%
  rename(Enrollments = n)

# Plotting the enrollment by year level
ggplot(year_level_enrollment, aes(x = YRLevel, y = Enrollments)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = 'Enrollment Variations by Student Year Level',
       x = 'Year Level',
       y = 'Number of Enrollments')
```
**Enrollments during the COVID period versus non-COVID periods**
```{r}
# Add a variable to indicate COVID period
filtered_enrollment <- filtered_enrollment %>%
  mutate(COVID_Period = ifelse(Sem >= 202002 & Sem <= 202105, "COVID", "Non-COVID"),
         Semester = ifelse(substr(Sem, 5, 6) == '02', 'Spring', 'Fall'))

# Calculate the total enrollments per semester within each COVID period
enrollments_per_semester <- filtered_enrollment %>%
  group_by(Sem, COVID_Period, Semester) %>%
  summarise(Total_Enrollments = n(), .groups = 'drop')

# Calculate the average enrollments for COVID and non-COVID periods by semester
covid_comparison_by_semester <- enrollments_per_semester %>%
  group_by(COVID_Period, Semester) %>%
  summarise(Average_Enrollments = mean(Total_Enrollments), .groups = 'drop')

# Plotting the comparison
ggplot(covid_comparison_by_semester, aes(x = Semester, y = Average_Enrollments, fill = COVID_Period)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = 'Comparing Enrollments by Semester: COVID Period vs Non-COVID Periods',
       x = 'Semester',
       y = 'Average Number of Enrollments') +
  scale_fill_manual(values = c("Non-COVID" = "blue", "COVID" = "red"))
```
The exploratory data analysis has provided several insights into the enrollment patterns:
\begin{enumerate}
\item Enrollment Trend over the Years: This graph provides an overview of how enrollment numbers have changed over time. We could tell normally there would be more student enrolling in elementary math courses in fall compared to in spring semester.
\item Courses by Enrollment: This graph displays the courses with the highest number of enrollments, identifying the most popular or required courses within the 100- and 200-level range would be calculus related courses.
\item Enrollment Trends by Primary Department: This chart illustrates the distribution of enrollments across different primary departments, indicating college of arts and sciences have the highest representation in these math courses. However, I found this a little uninformative as the LA would be the largest departments across the campus. Thus, the unbalanced case with no prior information about the department distribution across the campus may not be that trustful. 
\item Enrollment Variations by Student Year Level: The distribution of enrollments across different year levels is shown, highlighting year levels 1 or 2 are more likely to enroll in these courses as it is introductory courses.
\item Comparing Enrollments: COVID Period vs. Non-COVID Periods: This comparison reveals any significant changes in enrollment numbers during the COVID period compared to non-COVID periods, which can provide insights into the impact of the pandemic on course enrollments. However, though it may seems virtually significant, we may need more statistical testing to show the result. Also, it could be due to the reason for increasing size of the university instead of just becuase of the pandemic. 
\end{enumerate}

For the next step in the analysis, we will employ statistical methods to quantitatively assess the impact of various factors (like course number, department, year level, and COVID period) on enrollment. This will involve building a statistical model to understand the relationships between these factors and enrollment numbers. Given the nature of the data, a regression model or a similar approach might be appropriate to evaluate these relationships.

## Statistical Modeling (Course-level)

Let's first do a linear regression for the statistical modeling!

The first thing we do is to find the appropriate response variables. Given that the data is at the individual student enrollment level for each course, there are a couple of ways to construct the response variable:
\begin{enumerate}
1. Binary Response (Logistic Regression): You could create a binary variable indicating whether a student is enrolled in a course or not (1 for enrolled, 0 for not enrolled). However, this approach might not be suitable since the data seems to include only enrolled students.
2. Count Response (Poisson or Negative Binomial Regression): A more appropriate approach might be to aggregate the data at a course level for each semester and use the total number of enrollments in each course as the response variable. This type of data is well-suited for count models like Poisson or negative binomial regression.
\end{enumerate}
Hence, here I choose to aggregate data by semester to see how it is depends on the covariates.

So the first model I build is a fairly simple model to see how average enrollment in intro-level math courses would be different by semester. 
$$Enrollment_{t} = \beta_0 +\beta_1 * Year + \beta_2 * Semester + \epsilon_t $$
```{r}
# Aggregate data
average_enrollment <- enrollment_trend %>%
  group_by(Year, Semester) %>%
  summarise(Average_Enrollments = mean(Enrollments), .groups = 'drop')

average_enrollment$Year_Since_2017 <- average_enrollment$Year - 2017

# Then, rebuild the model with the transformed Year variable
model <- lm(Average_Enrollments ~ Year_Since_2017 + Semester, data = average_enrollment)

# View the updated summary of the model
summary(model)
```
So here, though I treated Year as a continuous variable and semester as a categorical variable, I still conduct some base group treatment for both year and semester. For year, I set 2017 as the baseline year, the intercept (constant term) in your regression model will represent the expected average enrollment for the year 2017 (assuming '2017' is transformed to '0'). This makes it much more intuitive and relevant since the intercept now corresponds to a specific, meaningful year in your dataset. 
The coefficient for Year_Since_2017 is 129.00. This means that, on average, each year since 2017 is associated with an increase of approximately 129 in average enrollments. This significant positive coefficient (p < 0.01) indicates a consistent yearly increase in enrollments.
The coefficient of -993.80 for SemesterSpring implies that enrollments in the Spring semester are, on average, about 994 less than in the Fall. This significant negative value (p < 0.001) suggests a strong seasonal effect, with lower enrollments in the Spring.
The model demonstrates a significant increase in average enrollments year over year since 2017, likely reflecting growing interest or increased availability of intro-level math courses. The substantial decrease in enrollments during the Spring semester could be attributed to various factors such as academic calendar structure, course offerings, or student preferences. The high R-squared value indicates a good fit for the model, but it's important to consider whether all relevant predictors have been included or if overfitting might be a concern given the high value.

```{r}
# Plot the model diagotics
par(mfrow = c(2, 2))
plot(model_time)

# Perform the Box-Cox transformation
boxcox_result <- boxcox(Average_Enrollments ~ Year_Since_2017 + Semester, 
                        data = average_enrollment)
```
While the diagnostic plots suggest that some assumptions may not be fully met, they do not show strong violations that would necessarily undermine the validity of the model. However, considering transformations or alternative modeling approaches could potentially improve the model fit and the reliability of its inferences. Hence, with the checking about box-cox transformation, we noticed the indicator function, meaning no change, would be good enough to give us a solid result.

The model provides a clear indication of temporal trends in enrollment, but it's essential to consider external factors that could influence these trends. Future models could explore additional predictors, like specific course attributes or broader educational trends, to gain more comprehensive insights.

SO first, we add the course factor to see how different course would holds different performance for enrollments

In this model, Year is treated as a continuous variable, Semester as a categorical variable, and Crs as a categorical variable. The model will estimate the effect of the Year, each Semester, and each Crs on the Enrollments. Here, the reference group is still 2017, Fall (largest group), 132 (largest group).

```{r}
course_enrollment_trend$Year_Since_2017 <- course_enrollment_trend$Year - 2017
course_enrollment_trend$Crs <- as.factor(course_enrollment_trend$Crs)
course_enrollment_trend$Crs <- relevel(course_enrollment_trend$Crs, ref = "132")

# Linear modeling
model_bycourse <- lm(Enrollments ~ Year_Since_2017 + Semester + Crs, data = course_enrollment_trend)

# Output the summary of the model
summary(model_bycourse)

# Plot the model diagotics
par(mfrow = c(2, 2))
plot(model_bycourse)
```
Interestingly, we found the coefficient for Year is not statistically significant (p > 0.05), indicating that there is no strong evidence of a year-over-year change in enrollments when controlling for semester and course. On the other hand, the coefficient for semester is significantly negative (p < 0.001), indicating that enrollments in the Spring semester are, on average, 132.871 less than in the Fall, suggesting a strong seasonal effect. The coefficients for other courses compared to course 132 show significant differences. For instance, course 100 has 435.81 fewer enrollments on average compared to course 132 (p < 0.001). This pattern holds for other courses as well, where negative coefficients indicate fewer enrollments compared to course 132. However, interestingly, the course 233, which is the Cal 3 and 132 is Cal 2, is the only one that is not significant. 

The model suggests semester timing (Spring vs. Fall) is a strong predictor of enrollment numbers, with fewer students enrolling in the Spring.
Specific courses have a significant impact on enrollment numbers, with most courses having fewer enrollments than course 132.
The year since 2017 does not appear to be a significant predictor of enrollment numbers when controlling for semester and course.

The lack of significance for the year could be due to multicollinearity, especially if certain courses are only offered in specific years or if there are other time-related variables that are not included in the model.
The model assumes a linear effect of time (Year) which may not be appropriate; enrollments could fluctuate in a non-linear fashion over time.
Given the number of courses included, there's a risk of the model being overfit. This could be addressed by cross-validation or other model selection techniques.
The significant intercept could be an artifact of how the baseline course (the reference category for Crs) is associated with enrollments.

Model deleting Year
```{r}
# delete Year
noyear_model <- lm(Enrollments ~ Semester + Crs, data = course_enrollment_trend)
summary(noyear_model)

# Interaction between Year and Semester
interaction_model <- lm(Enrollments ~ Year_Since_2017 * Semester + Crs, data = course_enrollment_trend)
summary(interaction_model)
```
The R-squared values are very similar across all models, suggesting that they all explain a similar amount of variance in enrollments.
The Adjusted R-squared values are also similar, indicating that the number of predictors used is appropriate for the amount of data.
The inclusion of Year in the interaction model does not add significant explanatory power compared to the first and no-year models. The interaction between Year and Semester is also not significant, suggesting that the effect of the semester on enrollments does not change over the years.
The inclusion of Year_Since_2017 in the model does not provide additional benefits in terms of model fit or explanatory power.
The semester consistently shows a significant effect across all models, indicating a seasonal pattern in enrollments.
Course numbers (Crs) are significant predictors in all models, demonstrating that enrollment numbers vary significantly by course.

Given the similarities in R-squared and Adjusted R-squared values, the simplest model (first model) might be preferred due to its parsimony. However, the decision should also consider the model's purpose, such as prediction or understanding individual predictors' effects.

Cross-Validation
```{r}
# install.packages("caret")
library(caret)

# Define control parameters for the cross-validation
control <- trainControl(method = "cv", number = 10)

# Fit the model using cross-validation
cv_model <- train(Enrollments ~ Semester + Crs, 
                  data = course_enrollment_trend, 
                  method = "lm", 
                  trControl = control)

# Summarize the cross-validation results
print(cv_model)
```
The model appears to be relatively good at explaining the variance in enrollments, with a high R-squared value.
The average errors (RMSE and MAE) seem to be moderately low, but without context (such as the range of enrollment values), it's hard to say definitively whether they are acceptable.
Since these are average metrics over 10 different folds, the model's performance is relatively stable across different subsets of the data, which is a good sign for its generalization capability.

Also, it is possible to consider the nonlinear models, which here I did a quick Poisson regression for that. 
```{r}
# Install the package if you haven't already
# install.packages("mgcv")
library(mgcv)

# Fit a GAM with potential nonlinear relationships
gam_model <- gam(Enrollments ~ s(Year, k = 6) + Semester + s(Crs, bs = "re"), 
                 method = "REML", 
                 data = course_enrollment_trend)

summary(gam_model)
```
It gives a fairly similar result showing our previous model may be good enough. 
```{r}
# Aggregate the enrollment data
department_course_enrollment <- filtered_enrollment %>%
  group_by(Year, Semester, Crs, PrimeDiv) %>%
  summarise(Total_Enrollments = n(), .groups = 'drop') %>%
  ungroup()

# Assuming PrimeDiv is already a factor. If not, convert it to a factor.
department_course_enrollment$PrimeDiv <- as.factor(department_course_enrollment$PrimeDiv)
department_course_enrollment$PrimeDiv <- relevel(department_course_enrollment$PrimeDiv, ref = "LA")

# Build the linear model including PrimeDiv as a factor
linear_model <- lm(Total_Enrollments ~ Year + Semester + Crs + PrimeDiv, data = department_course_enrollment)

# Output the summary of the model
summary(linear_model)

# Define control parameters for the cross-validation
control <- trainControl(method = "cv", number = 10)

# Fit the model using cross-validation
cv_model <- train(Total_Enrollments ~ Year + Semester + Crs + PrimeDiv, 
                  data = department_course_enrollment, 
                  method = "lm", 
                  trControl = control)

# Summarize the cross-validation results
print(cv_model)
```
Year: Similar to the previous model, the coefficient for Year is not statistically significant, suggesting that year-on-year changes in enrollment are not detected by the model when controlling for other factors.
SemesterSpring: The coefficient remains significantly negative but is now smaller in magnitude (-34.714 compared to -132.871 in the previous model). This indicates that, when accounting for the primary department, the impact of being in the Spring semester on enrollments is less pronounced than when the primary department is not included in the model.
Crs: The coefficients for different courses (Crs) remain significant, indicating that enrollments vary by course number, similar to the previous model.
PrimeDiv: The coefficients for different primary departments are significant, with negative values for PrimeDivBU, PrimeDivEN, and PrimeDivOD. This suggests that students from these departments are enrolled in fewer numbers compared to the baseline department.

Model Fit:
Residual Standard Error: The RSE has decreased to 42.94 from 56.19 in the previous model, suggesting better prediction accuracy with the inclusion of PrimeDiv.
R-squared: The R-squared value is 0.6022, which is lower than the previous model (0.8862). This suggests that this model with PrimeDiv explains a lower proportion of the variance in enrollments compared to the model without PrimeDiv.
Adjusted R-squared: It has also decreased to 0.5789, suggesting that some of the additional explanatory power of the previous model was due to the inclusion of a large number of predictors.

Comparing Models:
The inclusion of PrimeDiv has provided significant insights into how enrollments vary by department, which was not accounted for in the previous model.
The decreased R-squared value indicates that while PrimeDiv is important, the previous model captured more variability, likely due to the granularity provided by the course numbers. This might also suggest that PrimeDiv and Crs could be interacting in a way that is not captured by simply including both as additive factors.

Interpretation:
The primary department is a significant factor in predicting enrollments, and different departments have varying levels of influence.
The seasonal effect of the semester is confirmed but is less dramatic when considering the primary department.
Course numbers continue to be strong predictors, with specific courses attracting significantly different enrollment numbers.

```{r}
# Delete Year as the insignificant variable
linear_model_pri_woyear <- lm(Total_Enrollments ~ Semester + Crs + PrimeDiv, data = department_course_enrollment)
summary(linear_model_pri_woyear)

# Add interaction terms to the model
interaction_model <- lm(Total_Enrollments ~ Semester + Crs * PrimeDiv, data = department_course_enrollment)
summary(interaction_model)

# Set up cross-validation
set.seed(123)  # For reproducibility
folds <- createFolds(department_course_enrollment$Total_Enrollments, k = 10)
control <- trainControl("cv", index = folds)

# Fit the model using cross-validation
cv_model <- train(Total_Enrollments ~ Year + Semester + Crs * PrimeDiv,
                  data = department_course_enrollment,
                  method = "lm",
                  trControl = control)

# Summarize the cross-validation results
print(cv_model)
```
Here, with interaction, it would face some collinearity problem. Hence, I tried some penalized regression to see if it could better fit the model.  

```{r}
library(glmnet)

# Assuming 'department_course_enrollment' has your data and necessary predictors
# Prepare the matrix of predictors and the response variable vector
predictors <- model.matrix(Total_Enrollments ~ Semester + Crs * PrimeDiv, data = department_course_enrollment)[, -1]  # Removing intercept
response <- department_course_enrollment$Total_Enrollments

# Ridge Regression
set.seed(123)  # For reproducibility
cv_ridge <- cv.glmnet(predictors, response, alpha = 0, lambda = 10^seq(10, -2, length = 100))
best_lambda_ridge <- cv_ridge$lambda.min
ridge_model <- glmnet(predictors, response, alpha = 0, lambda = best_lambda_ridge)

# Lasso Regression
cv_lasso <- cv.glmnet(predictors, response, alpha = 1, lambda = 10^seq(10, -2, length = 100))
best_lambda_lasso <- cv_lasso$lambda.min
lasso_model <- glmnet(predictors, response, alpha = 1, lambda = best_lambda_lasso)

# Output the best lambda values
print(paste("Best lambda for Ridge:", best_lambda_ridge))
print(paste("Best lambda for Lasso:", best_lambda_lasso))

# Plot the coefficient paths
plot(cv_ridge)
plot(cv_lasso)

# Predict using the Ridge and Lasso models
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = predictors)
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = predictors)

# Calculate residuals
ridge_residuals <- response - ridge_predictions
lasso_residuals <- response - lasso_predictions

# Plot residuals
plot(ridge_residuals, main = "Ridge Regression Residuals")
plot(lasso_residuals, main = "Lasso Regression Residuals")

# Output the cross-validation results
print(cv_ridge)
print(cv_lasso)

# Predict the values using the model
predicted_values <- predict(cv_model, newdata = department_course_enrollment)

# Calculate the residuals (differences between observed and predicted values)
residuals <- department_course_enrollment$Total_Enrollments - predicted_values

# Calculate the Mean Squared Error (MSE)
mse <- mean(residuals^2)

# Output the MSE
print(mse)
```

The high MSE showing it may not be ideal to consider this model anyhow. And for the simplicity of interpretation, it may be better to ignore the interaction effect.





```{r}
# Calculate mean and variance
mean_enrollments <- mean(department_course_enrollment$Total_Enrollments)
variance_enrollments <- var(department_course_enrollment$Total_Enrollments)

# Check for overdispersion
overdispersion <- variance_enrollments > mean_enrollments
print(paste("Mean Enrollments:", mean_enrollments, 
            "Variance Enrollments:", variance_enrollments, 
            "Overdispersion:", overdispersion))

# Load necessary package
library(MASS)  # For Negative Binomial Model

# Define independent and response variables
X <- department_course_enrollment[, !colnames(department_course_enrollment) %in% c("Total_Enrollments", "Course_Semester")]
y <- department_course_enrollment$Total_Enrollments

# Fit the model based on overdispersion
if (overdispersion) {
  # Negative Binomial Model
  model_count <- glm.nb(Total_Enrollments ~ , data = department_course_enrollment)
} else {
  # Poisson Model
  model_count <- glm(Total_Enrollments ~ , family = poisson(), data = department_course_enrollment)
}

# Fitting the model and displaying the summary
summary(model_count)

par(mfrow = c(2, 2))
plot(model_count)
```

```{r}
# Creating a unique identifier for each course per semester
filtered_enrollment$Course_Semester <- paste(filtered_enrollment$Crs, filtered_enrollment$Sem, sep = "_")

# Aggregating Total Enrollments
course_semester_agg <- filtered_enrollment %>%
  group_by(Crs, Sem) %>%
  summarise(Total_Enrollments = n(), .groups = 'drop')

# Aggregating PrimeDiv, YRLevel, GradeOpt
prime_div_agg <- filtered_enrollment %>%
  mutate_at(vars(PrimeDiv), as.factor) %>%
  model.matrix(~ PrimeDiv + 0, data = .) %>%
  cbind(filtered_enrollment[, c("Crs", "Sem")]) %>%
  group_by(Crs, Sem) %>%
  summarise(across(starts_with("PrimeDiv"), sum), .groups = 'drop')%>%
  left_join(course_semester_agg, by = c("Crs", "Sem")) %>%
  mutate(across(starts_with("PrimeDiv"), ~ .x / Total_Enrollments)) 

prime_div_agg <- prime_div_agg[, !names(prime_div_agg) %in% 'Total_Enrollments']

yr_level_agg <- filtered_enrollment %>%
  mutate_at(vars(YRLevel), as.factor) %>%
  model.matrix(~ YRLevel + 0, data = .) %>%
  cbind(filtered_enrollment[, c("Crs", "Sem")]) %>%
  group_by(Crs, Sem) %>%
  summarise(across(starts_with("YRLevel"), sum), .groups = 'drop')%>%
  left_join(course_semester_agg, by = c("Crs", "Sem"))%>%
  mutate(across(starts_with("YRLevel"), ~ .x / Total_Enrollments))

yr_level_agg <- yr_level_agg[, !names(yr_level_agg) %in% 'Total_Enrollments']

grade_opt_agg <- filtered_enrollment %>%
  mutate_at(vars(GradeOpt), as.factor) %>%
  model.matrix(~ GradeOpt + 0, data = .) %>%
  cbind(filtered_enrollment[, c("Crs", "Sem")]) %>%
  group_by(Crs, Sem) %>%
  summarise(across(starts_with("GradeOpt"), sum), .groups = 'drop')%>%
  left_join(course_semester_agg, by = c("Crs", "Sem")) %>%
  mutate(across(starts_with("GradeOpt"), ~ .x / Total_Enrollments))

grade_opt_agg <- grade_opt_agg[, !names(grade_opt_agg) %in% 'Total_Enrollments']

# Merging Aggregated Data
enrollment_data_agg <- course_semester_agg %>%
  left_join(prime_div_agg, by = c("Crs", "Sem")) %>%
  left_join(yr_level_agg, by = c("Crs", "Sem")) %>%
  left_join(grade_opt_agg, by = c("Crs", "Sem"))

# Building the linear model without the Crs covariate
lm_model_count <- lm(Total_Enrollments ~ . - Crs - Sem, data = enrollment_data_agg)

# Displaying the summary of the model
summary(lm_model_count)

par(mfrow = c(2, 2))
plot(lm_model_count)
```


```{r}
# Calculate mean and variance
mean_enrollments <- mean(enrollment_data_agg$Total_Enrollments)
variance_enrollments <- var(enrollment_data_agg$Total_Enrollments)

# Check for overdispersion
overdispersion <- variance_enrollments > mean_enrollments
print(paste("Mean Enrollments:", mean_enrollments, 
            "Variance Enrollments:", variance_enrollments, 
            "Overdispersion:", overdispersion))

# Load necessary package
library(MASS)  # For Negative Binomial Model

# Define independent and response variables
X <- enrollment_data_agg[, !colnames(enrollment_data_agg) %in% c("Total_Enrollments", "Course_Semester")]
y <- enrollment_data_agg$Total_Enrollments

# Fit the model based on overdispersion
if (overdispersion) {
  # Negative Binomial Model
  model_count <- glm.nb(Total_Enrollments ~ . - Crs - Sem, data = enrollment_data_agg)
} else {
  # Poisson Model
  model_count <- glm(Total_Enrollments ~ . - Crs - Sem, family = poisson(), data = enrollment_data_agg)
}

# Fitting the model and displaying the summary
summary(model_count)

par(mfrow = c(2, 2))
plot(model_count)
```

## Statistical Modeling (Student Level)

```{r}
# Faceted Plot for Distribution of PrimeDiv in Each Course
ggplot(filtered_enrollment, aes(x = PrimeDiv, fill = PrimeDiv)) + 
  geom_bar() + 
  facet_wrap(~ Crs, scales = "free_x") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Distribution of Primary Departments Across Courses",
       x = "Primary Department",
       y = "Count")
```


```{r}
# Convert necessary columns to factors
filtered_enrollment_model <- filtered_enrollment %>%
  mutate(across(c(PrimeDiv, YRLevel, Sem, GradeOpt), as.factor))
# Convert course numbers to numeric for linear regression
filtered_enrollment_model$Crs <- as.numeric(as.character(filtered_enrollment_model$Crs))

lm_model_bycourse <- lm(Crs ~ PrimeDiv + YRLevel + Year + Semester, data=filtered_enrollment_model)

summary(lm_model_bycourse)

par(mfrow = c(2, 2))
plot(model)
```


```{r}
# Create dummy variables for categorical predictors
filtered_enrollment_model <- filtered_enrollment_model %>%
  mutate(Crs = as.factor(Crs))

# Create a data frame with dummy variables
model_data <- model.matrix(~ PrimeDiv + YRLevel + Sem + GradeOpt - 1, data = filtered_enrollment_model)
model_data <- as.data.frame(model_data)

# Add the response variable 'Crs' to the model data
model_data$Crs <- filtered_enrollment_model$Crs

# Fit the Multinomial Logistic Regression Model
model <- multinom(Crs ~ ., data = model_data)

# Coefficients
coefficients <- coef(model)

# Convert to a tidy data frame
(coef_df <- as.data.frame(coefficients))
```

```{r}
# Predictions using the same data used for fitting the model
fitted_probs <- predict(model, newdata = model_data, type = "probs")

# Convert predicted probabilities to class predictions
max_prob_indices <- max.col(fitted_probs, ties.method = "first")
predicted_classes <- levels(model_data$Crs)[max_prob_indices]

# Actual classes - ensure this is the same as used in the model
actual_classes <- model_data$Crs

# Creating the confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)
print(confusion_matrix)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))
```

## Pandemic Influence
```{r}
# Assuming 'filtered_data' is your R dataframe and 'Sem' is the semester column
# Define the start of the pandemic
start_pandemic_sem <- 202002

# Create a binary pandemic indicator
enrollment_data_agg$pandemic_period <- ifelse(enrollment_data_agg$Sem >= start_pandemic_sem, 'During', 'Before')

# Descriptive statistics
summary_statistics <- aggregate(Total_Enrollments ~ pandemic_period, data = enrollment_data_agg, FUN = mean)

# T-test or Wilcoxon test
result <- t.test(Total_Enrollments ~ pandemic_period, data = enrollment_data_agg)
# For non-parametric test, use: result <- wilcox.test(Enrollment ~ pandemic_period, data = enrollment_data_agg)

# Output result
print(result)
```

